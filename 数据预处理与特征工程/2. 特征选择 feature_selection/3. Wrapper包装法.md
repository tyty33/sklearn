# Wrapper包装法

包装法也是一个特征选择和算法训练同时进行的方法，与嵌入法十分相似，它也是依赖于算法自身的选择，比如coef_属性或feature_importances_属性来完成特征选择。但**不同的是，我们往往使用一个目标函数作为黑盒来帮助我们选取特征，而不是自己输入某个评估指标或统计量的阈值**。
包装法在初始特征集上训练评估器，并且通过coef_属性或通过feature_importances_属性获得每个特征的重要性。然后，从当前的一组特征中修剪最不重要的特征。在修剪的集合上递归地重复该过程，直到最终到达所需数量的要选择的特征。
区别于过滤法和嵌入法的一次训练解决所有问题，**包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的**。

![Untitled](Wrapper%E5%8C%85%E8%A3%85%E6%B3%95%2090913/Untitled.png)

注意，在这个图中的“算法”，指的不是我们最终用来导入数据的分类或回归算法（即不是随机森林），而是专业的数据挖掘算法，即我们的目标函数。这些数据挖掘算法的核心功能就是选取最佳特征子集。

最典型的目标函数是递归特征消除法（Recursive feature elimination, 简写为RFE）。它是一种贪婪的优化算法，旨在找到性能最佳的特征子集。 它反复创建模型，并在每次迭代时保留最佳特征或剔除最差特征，下一次迭代时，它会使用上一次建模中没有被选中的特征来构建下一个模型，直到所有特征都耗尽为止。 然后，它根据自己保留或剔除特征的顺序来对特征进行排名，最终选出一个最佳子集。**包装法的效果是所有特征选择方法中最利于提升模型表现的，它可以使用很少的特征达到很优秀的效果。除此之外，在特征数目相同时，包装法和嵌入法的效果能够匹敌，不过它比嵌入法算得更加缓慢，所以也不适用于太大型的数据。相比之下，包装法是最能保证模型效果的特征选择方法。**

feature_selection.RFE
*class sklearn.feature_selection.RFE (estimator, n_features_to_select=None, step=1, verbose=0)*

estimator是需要填写的实例化后的评估器，
n_features_to_select是想要选择的特征个数，
step表示每次迭代中希望移除的特征个数。

除此之外，RFE类有两个很重要的属性：
.support_：返回所有的特征的是否最后被选中的布尔矩阵，
.ranking_返回特征的按数次迭代中综合重要性的排名。

类feature_selection.RFECV会在交叉验证循环中执行RFE以找到最佳数量的特征，增加参数cv，其他用法都和RFE一模一样。

```python
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier as RFC
import pandas as pd
import numpy as np
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import os
from sklearn.feature_selection import RFE

os.chdir("F:\sklearn\Titanic")

data = pd.read_csv('./data.csv', index_col=0)
data = data.drop(['Name', 'Ticket', 'Cabin'], axis=1)

'''数据预处理'''

# 处理缺失值，对缺失值较多的列进行填补，有一些特征只确实一两个值，可以采取直接删除记录的方法
data['Age'] = data['Age'].fillna(data['Age'].mean())
data = data.dropna()

# 将分类变量转换为数值型变量

# 将二分类变量转换为数值型变量
# astype能够将一个pandas对象转换为某种类型，和apply(int(x))不同，astype可以将文本类转换为数字，用这个方式可以很便捷地将二分类特征转换为0~1
data['Sex'] = (data['Sex'] == 'male').astype('int')

# 将三分类变量转换为数值型变量
labels = data.Embarked.unique().tolist()
data['Embarked'] = data['Embarked'].apply(lambda x: labels.index(x))

'''实例化'''
X = data.iloc[:, data.columns != 'Survived']
y = data.iloc[:, data.columns == "Survived"]
print(X.columns)

y = y.values.ravel()
RFC_ = RFC(n_estimators=10, random_state=0)
selector = RFE(RFC_, n_features_to_select=5, step=1).fit(X, y)
print(selector.support_.sum())
print(selector.ranking_)
X_wrapper = selector.transform(X)
res = cross_val_score(RFC_, X_wrapper, y, cv=5).mean()
print(res)

'''画学习曲线来找n_features_to_select最佳阈值'''
score = []
for i in range(1, 8, 1):
    X_wrapper = RFE(RFC_, n_features_to_select=i, step=50).fit_transform(X, y)
    once = cross_val_score(RFC_, X_wrapper, y, cv=5).mean()
    score.append(once)
plt.figure(figsize=[20, 5])
plt.plot(range(1, 8, 1), score)
plt.xticks(range(1, 8, 1))
plt.show()
print(max(score))
```