# Embedded嵌入法

嵌入法是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行。在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。这些权值系数往往代表了特征对于模型的某种贡献或某种重要性，比如决策树和树的集成模型中的feature_importances_属性，可以列出各个特征对树的建立的贡献，我们就可以基于这种贡献的评估，找出对模型建立最有用的特征。因此**相比于过滤法，嵌入法的结果会更加精确到模型的效用本身，对于提高模型效力有更好的效果。**并且，由于考虑特征对模型的贡献，因此无关的特征（需要相关性过滤的特征）和无区分度的特征（需要方差过滤的特征）都会因为缺乏对模型的贡献而被删除掉，可谓是过滤法的进化版。

feature_selection.SelectFromModel
*class sklearn.feature_selection.SelectFromModel (estimator, threshold=None, prefit=False,norm_order=1,max_features=None)*

SelectFromModel是一个元变换器，可以与任何在拟合后具有coef_，feature_importances_属性或参数中可选惩罚项的评估器一起使用（比如随机森林和树模型就具有属性feature_importances_，逻辑回归就带有l1和l2惩罚项，线性支持向量机也支持l2惩罚项）。

对于有feature_importances_的模型来说，若重要性低于提供的阈值参数，则认为这些特征不重要并被移除。feature_importances_的取值范围是[0,1]，如果设置阈值很小，比如0.001，就可以删除那些对标签预测完全没贡献的特征。如果设置得很接近1，可能只有一两个特征能够被留下。

对于使用惩罚项的模型来说，正则化惩罚项越大，特征在模型中对应的系数就会越小。当正则化惩罚项大到一定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋于0。 但是我们会发现一部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。也就是说，我们选择特征系数较大的特征。另外，支持向量机和逻辑回归使用参数C来控制返回的特征矩阵的稀疏性，参数C越小，返回的特征越少。Lasso回归，用alpha参数来控制返回的特征矩阵，alpha的值越大，返回的特征越少。

| 参数 | 说明 |
| --- | --- |
| estimator | 使用的模型评估器，只要是带feature_importances_或者coef_属性，或带有l1和l2惩罚项的模型都可以使用 |
| threshold | 特征重要性的阈值，重要性低于这个阈值的特征都将被删除 |
| prefit | 默认False，判断是否将实例化后的模型直接传递给构造函数。如果为True，则必须直接调用fit和transform，不能使用fit_transform，并且SelectFromModel不能与cross_val_score，GridSearchCV和克隆估计器的类似实用程序一起使用。 |
| norm_order | k可输入非零整数，正无穷，负无穷，默认值为1在评估器的coef_属性高于一维的情况下，用于过滤低于阈值的系数的向量的范数的阶数。 |
| max_features | 在阈值设定下，要选择的最大特征数。要禁用阈值并仅根据max_features选择，请设置threshold = -np.inf |

在嵌入法下，我们很容易就能够实现特征选择的目标：减少计算量，提升模型表现。因此，比起要思考很多统计量的过滤法来说，嵌入法可能是更有效的一种方法。
然而，在算法本身很复杂的时候，过滤法的计算远远比嵌入法要快，所以大型数据中，我们还是会优先考虑过滤法。

```python
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier as RFC
import pandas as pd
import numpy as np
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import os

os.chdir("F:\sklearn\Titanic")

data = pd.read_csv('./data.csv', index_col=0)
data = data.drop(['Name', 'Ticket', 'Cabin'], axis=1)

'''数据预处理'''

# 处理缺失值，对缺失值较多的列进行填补，有一些特征只确实一两个值，可以采取直接删除记录的方法
data['Age'] = data['Age'].fillna(data['Age'].mean())
data = data.dropna()

# 将分类变量转换为数值型变量

# 将二分类变量转换为数值型变量
# astype能够将一个pandas对象转换为某种类型，和apply(int(x))不同，astype可以将文本类转换为数字，用这个方式可以很便捷地将二分类特征转换为0~1
data['Sex'] = (data['Sex'] == 'male').astype('int')

# 将三分类变量转换为数值型变量
labels = data.Embarked.unique().tolist()
data['Embarked'] = data['Embarked'].apply(lambda x: labels.index(x))

'''实例化'''
X = data.iloc[:, data.columns != 'Survived']
y = data.iloc[:, data.columns == "Survived"]
print(X.shape)
RFC_ = RFC(n_estimators=10, random_state=0)

'''取出有限的特征。'''
y = y.values.ravel()  # 当需要一维数组时，传递了列向量y。请将Y的形状更改为（n_samples）,例如使用.ravel()
X_embedded = SelectFromModel(RFC_, threshold=0.082).fit_transform(X, y)
print(X_embedded.shape)
res = cross_val_score(RFC_, X_embedded, y, cv=5).mean()
print(res)

'''画学习曲线来找threshold最佳阈值'''

feature_importance = RFC_.fit(X, y).feature_importances_
print(feature_importance)

threshold = np.linspace(0, feature_importance.max(), 20)
score = []
for i in threshold:
    X_embedded = SelectFromModel(RFC_, threshold=i).fit_transform(X, y)
    once = cross_val_score(RFC_, X_embedded, y, cv=5).mean()
    score.append(once)
plt.plot(threshold, score)
plt.show()
print(max(score))
```