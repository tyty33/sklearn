# 分类树 DecisionTreeClassifier

为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标叫做“不纯度”。通常来说，不纯度越低，决策树对训练集的拟合越好。现在使用的决策树算法在分枝方法上的核心大多是围绕在对某个不纯度相关指标的最优化上。

不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是说，在同一棵决策树上，叶子节点的不纯度一定是最低的。

```python
from sklearn import tree
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
import pandas as pd
import graphviz
import matplotlib.pyplot as plt

# 以dataframe形式查看数据
pd.concat([pd.DataFrame(wine.data),pd.DataFrame(wine.target)],axis = 1)

wine.feature_names
wine.target_names
```

# 重要参数

1. criterion

Criterion这个参数正是用来决定不纯度的计算方法的。sklearn提供了两种选择：
1）”entropy“，使用信息熵（Entropy）
2）”gini“，使用基尼系数（Gini Impurity）

比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是在实际使用中，信息熵和基尼系数的效果基本相同。信息熵的计算比基尼系数缓慢一些，因为基尼系数的计算不涉及对数。另外，因为信息熵对不纯度更加敏感，所以信息熵作为指标时，决策树的生长会更加“精细”，因此**对于高维数据或者噪音很多的数据，信息熵很容易过拟合，基尼系数在这种情况下效果往往比较好。当模型拟合程度不足的时候，即当模型在训练集和测试集上都表现不太好的时候，使用信息熵。**当然，这些不是绝对的。

选取参数：
通常就使用基尼系数
数据维度很大，噪音很大时使用基尼系数
维度低，数据比较清晰的时候，信息熵和基尼系数没区别
当决策树的拟合程度不够的时候，使用信息熵
两个都试试，不好就换另外一个

1. random_state & splitter

random_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。

splitter也是用来控制决策树中的随机选项的，有两种输入值。
输入”best"，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），
输入“random"，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这也是防止过拟合的一种方式。

1. 剪枝参数

max_depth：限制树的最大深度，超过设定深度的树枝全部剪掉

这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效果再决定是否增加设定深度。

min_samples_leaf & min_samples_split

min_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生。一般搭配max_depth使用，一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。

min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。

max_features & min_impurity_decrease

一般配合max_depth使用，用作树的”精修“

max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。

min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。

1. 目标权重分数

class_weight & min_weight_fraction_leaf

使用class_weight参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。

有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分

```python
# 划分训练集和测试集
Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data, wine.target,test_size=0.3)

# 实例化、建立模型
clf = tree.DecisionTreeClassifier(criterion='entropy'
                                 ,random_state=30 
                                 ,splitter='random'
                                 ,max_depth=3 
                                 ,min_samples_leaf=10 
                                 ,min_samples_split=10 
                                 ) 
                                  
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest,Ytest)

# 画树
feature_name = ['酒精','苹果酸','灰','灰的碱性','镁','总酚','类黄酮','非黄烷类酚类','花青素','颜色强度','色调','od280/od315稀释葡萄酒','脯氨酸']

dot_data = tree.export_graphviz(clf,
                               feature_names=feature_name,
                               class_names=['琴酒','雪莉','贝尔摩德'],
                               filled = True,
                               rounded = True)

graph = graphviz.Source(dot_data)
graph

# 查看特征重要性
[*zip(feature_name,clf.feature_importances_)]

# 测试集的拟合程度
score_train = clf.score(Xtest,Ytest)
score_train

# 使用确定超参数的曲线确认最优的剪枝参数
test = []

for i in range(10):
    clf = tree.DecisionTreeClassifier(max_depth= i + 1
                                     ,criterion='entropy'
                                     ,random_state=30)
    
    clf = clf.fit(Xtrain,Ytrain)
    score = clf.score(Xtest,Ytest)
    test.append(score)
plt.plot(range(1,11),test,color='red',label='max_depth')
plt.legend()
plt.show()

# apply 返回每个测试样本所在的叶子节点的索引
clf.apply(Xtest)

# predict 返回每个测试样本的分类/回归结果
clf.predict(Xtest)
```