# 回归树 DecisionTreeRegressor

在回归树中，没有标签分布是否均衡的问题，因此没有class_weight这样的参数

回归树衡量分枝质量的指标，支持的标准有三种：
1）输入"mse"使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失
2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差
3）输入"mae"使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失

虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误
差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。

交叉验证是用来观察模型的稳定性的一种方法，我们将数据划分为n份，依次使用其中一份作为测试集，其他n-1份作为训练集，多次计算模型的精确性来评估模型的平均准确程度。训练集和测试集的划分会干扰模型的结果，因此用交叉验证n次的结果求出的平均值，是对模型效果的一个更好的度量。

```python
#交叉验证cross_val_score
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeRegressor

boston = load_boston()
regressor = DecisionTreeRegressor(random_state=0)
cross_val_score(regressor, boston.data, boston.target, cv=10, 
                scoring = "neg_mean_squared_error")
```

```python
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

#创建一条含有噪声的正弦曲线

#生成随机数种子
rng = np.random.RandomState(1)
#生成80乘1的升序矩阵，升序由axis = 0控制
X = np.sort(5 * rng.rand(80,1),axis=1)
#使用ravel降维
y =  np.sin(X).ravel()
y[::5] += 3 *(0.5 - rng.rand(16))
plt.figure()
plt.scatter(X,y,s=20)

''''''

regr_1 = DecisionTreeRegressor(max_depth=2)
regr_2 = DecisionTreeRegressor(max_depth=5)
regr_1.fit(X,y)
regr_2.fit(X,y)

#使用np.newaxis来增维
#l[:,np.newaxis]把l增维
#l[np.newaxis,:]把l降维

X_test = np.arange(0.0,5.0,0.01)[:,np.newaxis]
X_test

#predict对每一个X数据求出对应 回归或者分类结果
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)

plt.figure()#建立一个画布
plt.scatter(X,y,s = 20,edgecolor = "black",c = "darkorange",label = "data")#画散点图
#化预测结果的折线图
plt.plot(X_test,y_1,label="max_depth=2",linewidth=2)
plt.plot(X_test,y_2,color = "yellowgreen",label="max_depth=5",linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("Decision Tree Regressor")
plt.legend()
plt.show()
```